import pandas as pd
import numpy as np
import random
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

class DegradationRL:
    def __init__(self, predict_df = predict_df, rul_df = rul_df):
        self.thresholds = {"A":15, "B":30, "C":45, "D":60, "E":100}
        self.max_age = 80
        self.fail_threshold = 60
        self.degradation_levels = ["A", "B", "C", "D", "E"]
        self.age_levels = [10, 20, 30, 40, 50, 60, 70, 80]
        self.num_states = self.calculate_num_states()
        self.action_space = ['Do Nothing', 'Replace', 'Repair 1', 'Repair 2', 'Repair 3']
        self.num_actions = len(self.action_space)
        self.dl = "A"
        self.age = 0
        self.costs = self.define_costs()
        self.initial_state = ("A", 0)
        self.failure_state = ("E", 80)
        self.current_state = self.initial_state
        self.current_state_indx = 0
        self.predict_df = predict_df.copy()
        self.rul_df = rul_df.copy()



    def predict_next_level(self, age, dl):
        if dl == 'A' and age == 0:
          return 'A'

        else:
            row = self.predict_df[
                (self.predict_df["level_now"] == dl) &
                (self.predict_df["age_now"] == age)]

            if row.empty:
                return dl
            else:
                return row.iloc[0]["predicted_next_level"]



    def define_costs(self):
        do_nothing_cost = 100
        preventive_replacement_cost = 5100
        corrective_replacement_cost = 15100

        repair1_costs = {("B", 10): 213.3, ("B", 20): 226.67, ("B", 30): 240, ("B", 40): 253.3,
                         ("B", 50): 266.67, ("B", 60): 280, ("B", 70): 293.3, ("B", 80): 306.67,
                         ("C", 10): 230, ("C", 20): 260, ("C", 30): 290, ("C", 40): 320,
                         ("C", 50): 350, ("C", 60): 380, ("C", 70): 410, ("C", 80): 440,
                         ("D", 10): 253.3, ("D", 20): 306.67, ("D", 30): 360, ("D", 40): 413.3,
                         ("D", 50): 466.67, ("D", 60): 520, ("D", 70): 573.3, ("D", 80): 626.67}

        repair2_costs = {("C", 10): 320, ("C", 20): 440, ("C", 30): 560, ("C", 40): 680,
                         ("C", 50): 800, ("C", 60): 920, ("C", 70): 1040, ("C", 80): 1160,
                         ("D", 10): 413.3, ("D", 20): 626.67, ("D", 30): 840, ("D", 40): 1053.3,
                         ("D", 50): 1266.67, ("D", 60): 1480, ("D", 70): 1693.3, ("D", 80): 1906.67}

        repair3_costs = {("D", 10): 680, ("D", 20): 1160, ("D", 30): 1640, ("D", 40): 2120,
                         ("D", 50): 2600, ("D", 60): 3080, ("D", 70): 3560, ("D", 80): 4040}

        costs = {
        'Do Nothing': do_nothing_cost,
        'Preventive Replacement': preventive_replacement_cost,
        'Corrective Replacement': corrective_replacement_cost,
        'Repair 1': repair1_costs,
        'Repair 2': repair2_costs,
        'Repair 3': repair3_costs
        }

        return costs



    def discretize_deg(self, degradation):
        if degradation >= 60:
            return "E"
        elif degradation >= 45:
            return "D"
        elif degradation >= 30:
            return "C"
        elif degradation >= 15:
            return "B"
        else:
            return "A"

    def discretize_age(self, age):
        if age <= 10:
            return 10
        elif age <= 20:
            return 20
        elif age <= 30:
            return 30
        elif age <= 40:
            return 40
        elif age <= 50:
            return 50
        elif age <= 60:
            return 60
        elif age <= 70:
            return 70
        else:
            return 80

    def get_state(self, dl, age):
        if age == 0 and dl == "A":
            return self.initial_state
        elif dl == "E":
           return self.failure_state
        else:
            degradation_level = dl
            discretized_age = self.discretize_age(age)
            return (degradation_level, discretized_age)


    def get_state_index(self, state):
        if state == self.initial_state:
          return 0
        elif state == self.failure_state:
          return 33
        else:
          degradation_level = state[0]
          discretized_age = state[1]
          index_age = self.age_levels.index(discretized_age)
          index_degredation_level = self.degradation_levels.index(degradation_level)
          state_index =  index_degredation_level*len(self.age_levels) + index_age + 1
          return state_index


    def get_available_actions(self):
        current_degradation_level = self.dl
        available_actions = ['Do Nothing', 'Replace']

        if current_degradation_level != "A":
            available_actions.append('Repair 1')
        if current_degradation_level not in ["A", "B"]:
            available_actions.append('Repair 2')
        if current_degradation_level not in ["A", "B", "C"]:
            available_actions.append('Repair 3')
        if current_degradation_level == "E":
            available_actions = ['Replace']
        return available_actions


    def improve_degradation_level(self, dl, improvement_steps):
        levels = self.degradation_levels
        current_level = dl
        current_index = levels.index(current_level)
        new_index = max(0, current_index - improvement_steps)
        new_level = levels[new_index]
        return new_level


    def calculate_num_states(self):
        num = len(self.degradation_levels) * len(self.age_levels)
        return num - 6


    def step(self, action):
        self.current_state = self.get_state(self.dl, self.age)
        if self.dl == "E":
          self.current_state = self.failure_state
        # print(f"Current State: {self.current_state}")
        # print(f"Current State Index: {self.current_state_indx}")
        reward = 0
        done = False
        if self.dl != "E" and self.age <= 80:
            if action == "Do Nothing":
                cost = self.costs['Do Nothing']

            elif action == "Replace":
                cost = self.costs['Preventive Replacement']
                self.reset()
                #done = True
                # print("Preventive Replacement is executed!")
            elif action == "Repair 1":
                cost = self.costs[action].get(self.current_state)
                self.dl = self.improve_degradation_level(self.dl, 1)
                self.current_state = self.get_state(self.dl, self.age)
            elif action == "Repair 2":
                cost = self.costs[action].get(self.current_state)
                self.dl = self.improve_degradation_level(self.dl, 2)
                self.current_state = self.get_state(self.dl, self.age)
            elif action == "Repair 3":
                cost = self.costs[action].get(self.current_state)
                self.dl = self.improve_degradation_level(self.dl, 3)
                self.current_state = self.get_state(self.dl, self.age)

            # print(f"After Maintenace State:{self.current_state}")

        else:
          cost = self.costs['Corrective Replacement']
          self.reset()
          done = True
          # print("Corrective Replacement is executed!")
          # print(f"After Maintenace State:{self.current_state}")

        rul = rul_df.loc[self.dl, self.age]
        reward = rul / cost
        self.dl = self.predict_next_level(self.age, self.dl)
        self.age += 10
        # print(f'next dl: {self.dl}')
        self.current_state = self.get_state(self.dl, self.age)
        self.current_state_indx = self.get_state_index(self.current_state)

        # print(f"RUL: {rul}")
        # print(f"Cost: {cost}")
        # print(f"Reward: {reward}")
        # print(f"Degradation Level: {self.dl}")
        # print(f"Next State: {self.current_state}")
        # print(f"Next State Index: {self.current_state_indx}")

        return self.current_state_indx, self.current_state, reward, done

    def reset(self):
        self.dl = "A"
        self.age = 0
        self.current_state = self.initial_state
        self.current_state_indx = 0

    def starting_state(self):
        self.dl = random.choice(self.degradation_levels)
        self.age = random.choice(self.age_levels)
        self.current_state = self.get_state(self.dl, self.age)
        self.current_state_indx = self.get_state_index(self.current_state)



def q_learning(env, alpha=0.1, gamma=0.9, epsilon_start=1, epsilon_decay=0.999, num_episodes=500, max_steps=50):
    q_table = np.zeros((env.num_states, env.num_actions))
    episode_rewards = []
    sa_visits = np.zeros((env.num_states, env.num_actions), dtype=int)
    epsilon = epsilon_start

    for episode in range(num_episodes):
        env.starting_state()
        #env.reset()
        state = env.current_state
        state_indx = env.get_state_index(state)
        done = False
        total_reward = 0
        step_count = 0

        while step_count < max_steps:
            # Epsilon-greedy action selection
            available_actions = env.get_available_actions()
            if random.uniform(0, 1) < epsilon:
                action = random.choice(available_actions)
            else:
                action_values = [q_table[state_indx, env.action_space.index(a)] for a in available_actions]
                action = available_actions[np.argmax(action_values)]

            action_index = env.action_space.index(action)

            # Count (state, action) visit
            sa_visits[state_indx, action_index] += 1

            # Take step
            next_state_indx, next_state, reward, done = env.step(action)
            total_reward += reward

            # Q-learning update
            old_value = q_table[state_indx, action_index]
            next_max = np.max(q_table[next_state_indx])
            q_table[state_indx, action_index] = old_value + alpha * (reward + gamma * next_max - old_value)

            # Move to next state
            state = next_state
            state_indx = next_state_indx
            step_count += 1

        if episode % 3 == 0 and episode > 0:
            # decay = (0.25 * epsilon) / (0.5 * episode)
            # epsilon = max(0.05, epsilon - decay)
            epsilon = max(0, epsilon * epsilon_decay)

        if episode % 100 == 0:
            print(f"Episode: {episode}, Total Reward: {total_reward}, eps: {epsilon}, step:{step_count}, Total/step:{total_reward/step_count}")
        episode_rewards.append(total_reward)

    return q_table, episode_rewards, sa_visits


max_age = 80
env = DegradationRL(predict_df = predict_df, rul_df = rul_df)
Q_table, episode_rewards, state_action_visits = q_learning(env, alpha=0.1, gamma=0.9, epsilon_start=1, epsilon_decay=0.999, num_episodes=30000, max_steps=100)

dl_levels = ['A', 'B', 'C', 'D']
age_levels = [10, 20, 30, 40, 50, 60, 70, 80]
num_dl_levels = len(dl_levels)
num_age_levels = len(age_levels)
num_states = num_dl_levels * num_age_levels + 2

states = [("A", 0)]
for dl in dl_levels:
    for age in age_levels:
        states.append((dl, age))
states.append(("E", 80))

for i in range(num_states):
    state = states[i]
    q_values = Q_table[i]

    state_str = f"({state[0]}, {state[1]})"
    q_values_str = "[" + ", ".join([f"{q:5.1f}" for q in q_values]) + "]"
    print(f"{state_str:<15}\t{q_values_str}")

dl_levels = ['A', 'B', 'C', 'D']
age_levels = [10, 20, 30, 40, 50, 60, 70, 80]
num_dl_levels = len(dl_levels)
num_age_levels = len(age_levels)
num_states = num_dl_levels * num_age_levels + 2

states = [("A", 0)]
for dl in dl_levels:
    for age in age_levels:
        states.append((dl, age))
states.append(("E", 80))

actions = env.action_space

data = []

for state_index, state in enumerate(states):
    row = {}
    for action_index, action in enumerate(actions):
        row[action] = state_action_visits[state_index, action_index]
    state_str = f"({state[0]}, {state[1]})"
    data.append(pd.Series(row, name=state_str))

visit_data = pd.DataFrame(data)
print(visit_data.to_string())

import matplotlib.pyplot as plt

def plot_rewards(episode_rewards, window=10):
    plt.figure(figsize=(10, 5))
    plt.plot(episode_rewards, label='Avg reward per step')

    # میانگین متحرک برای صاف‌تر شدن منحنی
    if len(episode_rewards) >= window:
        rolling = pd.Series(episode_rewards).rolling(window=window).mean()
        plt.plot(rolling, label=f'{window}-episode moving average', linestyle='--')

    plt.xlabel('Episode')
    plt.ylabel('Average Reward per Step')
    plt.title('Average Reward per Step over Episodes')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
plot_rewards(episode_rewards)

for s in range(34):
  map=['Do Nothing', 'Replace', 'Repair 1', 'Repair 2', 'Repair 3']
  print("in {} the best action is:{}".format(states[s], map[np.argmax(Q_table[s])] ))


# Comprasion

action_names = ['Do Nothing', 'Replace', 'Repair 1', 'Repair 2', 'Repair 3']
action_to_name = {i: name for i, name in enumerate(action_names)}
name_to_action = {name: i for i, name in enumerate(action_names)}
env = DegradationRL(predict_df = predict_df, rul_df = rul_df)

def q_policy(state, q_table, env):
    state_idx = env.get_state_index(state)
    best_action_idx = np.argmax(q_table[state_idx])
    return action_names[best_action_idx]

scenario1_policy = {
    ('A', 10): 'Do Nothing', ('A', 20): 'Do Nothing', ('A', 30): 'Do Nothing',
    ('A', 40): 'Do Nothing', ('A', 50): 'Do Nothing', ('A', 60): 'Do Nothing',
    ('A', 70): 'Replace', ('A', 80): 'Replace',

    ('B', 10): 'Repair 1', ('B', 20): 'Repair 1', ('B', 30): 'Repair 1',
    ('B', 40): 'Repair 1', ('B', 50): 'Repair 1', ('B', 60): 'Do Nothing',
    ('B', 70): 'Replace', ('B', 80): 'Replace',

    ('C', 10): 'Repair 2', ('C', 20): 'Repair 2', ('C', 30): 'Repair 2',
    ('C', 40): 'Repair 2', ('C', 50): 'Repair 2', ('C', 60): 'Repair 1',
    ('C', 70): 'Replace', ('C', 80): 'Replace',

    ('D', 10): 'Repair 3', ('D', 20): 'Repair 3', ('D', 30): 'Repair 3',
    ('D', 40): 'Repair 3', ('D', 50): 'Repair 3', ('D', 60): 'Repair 3',
    ('D', 70): 'Replace', ('D', 80): 'Replace',

    ('E', 70): 'Replace', ('E', 80): 'Replace'
}


lazy_policy = {
    (dl, age): 'Do Nothing' for dl in ['A', 'B', 'C', 'D'] for age in range(10, 90, 10)
}
lazy_policy.update({('E', age): 'Replace' for age in range(10, 90, 10)})


def run_policy(env, policy_fn, episodes=2500, max_steps=100, q_table=None):
    rewards = []
    for ep in range(episodes):
        env.starting_state()
        total_reward = 0
        for _ in range(max_steps):
            state = env.get_state(env.dl, env.age)
            if callable(policy_fn):
                action_name = policy_fn(state, q_table, env)
            else:
                action_name = policy_fn.get(state, 'Do Nothing')

            next_state_idx, next_state, reward, done = env.step(action_name)
            total_reward += reward
            if done:
                break
        rewards.append(total_reward)
    return rewards



rewards_q = run_policy(env, q_policy, q_table=Q_table)
rewards_scenario1 = run_policy(env, scenario1_policy)
rewards_lazy = run_policy(env, lazy_policy)


print("Average Reward - Q-learning:    ", np.mean(rewards_q))
print("Average Reward - Scenario 1:    ", np.mean(rewards_scenario1))
print("Average Reward - Lazy policy:   ", np.mean(rewards_lazy))
 
